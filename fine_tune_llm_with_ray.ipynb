{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this challenge, we are tasked with the development of AI systems capable of receiving textual descriptions as input and generating high-quality audio wave files as output. These AI systems will craft customized background music, considering various elements such as melody, hits, styles, and more to evoke the intended emotional and contextual resonance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using example from this website:\n",
    "https://docs.ray.io/en/latest/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install ray\n",
    "!pip3 install datasets\n",
    "!pip3 install transformer\n",
    "!pip3 install numpy datasets \"transformers>=4.19.1\" \"pytorch_lightning>=1.6.5\"\n",
    "!pip3 install accelerate\n",
    "!pip3 install lightning\n",
    "!pip3 install deepspeed\n",
    "\n",
    "!pip install miditok\n",
    "!pip install miditoolkit\n",
    "!pip install torch\n",
    "!pip install torchtoolkit\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import Tensor, argmax\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda import is_available as cuda_available, is_bf16_supported\n",
    "from torch.backends.mps import is_available as mps_available\n",
    "from torchtoolkit.data import create_subsets\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
    "from evaluate import load as load_metric\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetTok, DataCollator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 16:02:05,986\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import re\n",
    "import ray\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForQuestionAnswering\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE_PER_WORKER = 8\n",
    "MODEL_NAME = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"pip\": [\n",
    "            \"datasets==2.13.1\",\n",
    "            \"torch>=1.13.0\",\n",
    "            \"deepspeed==0.9.4\",\n",
    "            \"accelerate>=0.20.3\",\n",
    "            \"transformers==4.30.2\",\n",
    "            \"lightning==2.0.3\",\n",
    "        ],\n",
    "    },\n",
    "    ignore_reinit_error=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get dataset from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Create a list of dictionaries where each dictionary represents a sample in the dataset\n",
    "with open(\"/Users/khoavo2003/Documents/GitHub/muze/new_train.json\", 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# reformat data\n",
    "dict_dataset = {\"description\":[], \"url\": []}\n",
    "for info in data.values():\n",
    "  dict_dataset['description'].append(info['description'])\n",
    "  dict_dataset['url'].append(info['url'])\n",
    "\n",
    "# Create a datasets.Dataset instance\n",
    "# You can also specify additional metadata such as features and split\n",
    "my_dataset = Dataset.from_dict(dict_dataset, split='train')\n",
    "\n",
    "# Save the dataset to a file (optional)\n",
    "# dataset_dict.save_to_disk(\"my_dataset\")\n",
    "\n",
    "# Load the dataset from the saved file\n",
    "# loaded_dataset = DatasetDict.load_from_disk(\"my_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['description', 'url'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess input and label to appropriate format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MidiTok config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing MIDIs (tmp): 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer's configuration\n",
    "PITCH_RANGE = (21, 109)\n",
    "BEAT_RES = {(0, 1): 8, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "NB_VELOCITIES = 24\n",
    "SPECIAL_TOKENS = [\"PAD\", \"MASK\", \"BOS\", \"EOS\"]\n",
    "USE_CHORDS = False\n",
    "USE_RESTS = False\n",
    "USE_TEMPOS = True\n",
    "USE_TIME_SIGNATURE = False\n",
    "USE_PROGRAMS = False\n",
    "NB_TEMPOS = 32\n",
    "TEMPO_RANGE = (50, 200)  # (min_tempo, max_tempo)\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": PITCH_RANGE,\n",
    "    \"beat_res\": BEAT_RES,\n",
    "    \"nb_velocities\": NB_VELOCITIES,\n",
    "    \"special_tokens\": SPECIAL_TOKENS,\n",
    "    \"use_chords\": USE_CHORDS,\n",
    "    \"use_rests\": USE_RESTS,\n",
    "    \"use_tempos\": USE_TEMPOS,\n",
    "    \"use_time_signatures\": USE_TIME_SIGNATURE,\n",
    "    \"use_programs\": USE_PROGRAMS,\n",
    "    \"nb_tempos\": NB_TEMPOS,\n",
    "    \"tempo_range\": TEMPO_RANGE,\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "tokenizer = REMI(config)  # REMI tokenizer\n",
    "tokenizer.tokenize_midi_dataset('we_wish_you.mid', 'tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditoolkit import MidiFile\n",
    "midi = MidiFile('we_wish_you.mid')\n",
    "tokens = tokenizer(midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_params(out_path='tmp/', filename='tokenizer_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = REMI(params=Path('tmp/tokenizer_config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: include the tokenizer for input and output, and code to get url\n",
    "tokenizer_input = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer_output = AutoTokenizer.from_pretrained\n",
    "def preprocess_input_output(input: str, output: str):\n",
    "  '''\n",
    "  function to preprocess input based on input and output tokenizer\n",
    "  args:\n",
    "    - input(str): string need to encode (description in this case)\n",
    "    - output(str): url to the music\n",
    "  '''\n",
    "  input_token = tokenizer_input.tokenize(input)\n",
    "  input_preprocess = tokenizer_input.convert_tokens_to_ids(input_token)\n",
    "  #TODO: include code to get data from url, change it to appropriate format, and get preprocess output\n",
    "  output_token = tokenizer_output.tokenize(output)\n",
    "  output_preprocess = tokenizer_output.convert_tokens_to_ids(output_token)\n",
    "  return input_preprocess, output_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = my_dataset.map(\n",
    "    lambda example: preprocess_input_output(example['description'], example['url']),\n",
    "    batched=True,\n",
    "    remove_columns=['description', 'url'],  # Remove the original columns\n",
    ")\n",
    "# TODO: Rename columns to 'input_ids' and 'labels'\n",
    "processed_dataset = processed_dataset.rename_column(\"0\", \"input_ids\")\n",
    "processed_dataset = processed_dataset.rename_column(\"1\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = processed_dataset.train_test_split(train_size=0.8, seed=20)\n",
    "processed_dataset[\"validation\"] = processed_dataset.pop(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train and evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change audio data into format that can be used to train and evaluate the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize music \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer_input, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    processed_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator, num_workers=4, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    processed_dataset[\"validation\"], batch_size=8, collate_fn=data_collator, num_workers=4, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSpeed Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"result\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"valid\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ray\n",
    "import lightning.pytorch as pl\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import (\n",
    "    init_empty_weights,\n",
    "    infer_auto_device_map,\n",
    "    load_checkpoint_and_dispatch,\n",
    ")\n",
    "\n",
    "# Initialize a model on meta device\n",
    "with init_empty_weights():\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    meta_model = AutoModelForCausalLM.from_config(config)\n",
    "meta_model.tie_weights()\n",
    "\n",
    "# Define the device mapping\n",
    "device_map = infer_auto_device_map(\n",
    "    meta_model,\n",
    "    max_memory={0: \"15GB\", \"cpu\": \"60GB\"},\n",
    "    no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    ")\n",
    "\n",
    "# Load the model parameters\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    meta_model,\n",
    "    checkpoint=full_model_ckpt_path,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    device_map=device_map,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME, padding_side=\"left\", use_fast=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strach (for testing code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tempo: 184.57 beats per minute\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Get the file path to an included audio example\n",
    "filename = \"we_wish_you.mp3\"\n",
    "\n",
    "# Load the audio as a waveform `y`\n",
    "# Store the sampling rate as `sr`\n",
    "y, sr = librosa.load(filename)\n",
    "\n",
    "# Run the default beat tracker\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "# Convert the frame indices of beat events into timestamps\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "\n",
    "# Function to play audio when the button is clicked\n",
    "clear_output(wait=True)  # Clear the previous output, if any\n",
    "display(Audio(y, rate=sr))  # Play the audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2023/09/text-to-sound-train-your-large-language-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
