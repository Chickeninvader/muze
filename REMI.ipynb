{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, TokenizerConfig  # here we choose to use REMI\n",
    "\n",
    "# Our parameters\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": (21, 109),\n",
    "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "    \"nb_velocities\": 32,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "    \"use_chords\": True,\n",
    "    \"use_rests\": False,\n",
    "    \"use_tempos\": True,\n",
    "    \"use_time_signatures\": False,\n",
    "    \"use_programs\": False,\n",
    "    \"nb_tempos\": 32,  # nb of tempo bins\n",
    "    \"tempo_range\": (40, 250),  # (min, max)\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Creates the tokenizer\n",
    "tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\muze\\REMI.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokens_bpe_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tokens_bpe\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode_bpe(tokens_bpe_list)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m midi \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtokens_to_midi(tokens)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\miditok\\midi_tokenizer.py:138\u001b[0m, in \u001b[0;36m_in_as_seq.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m tokenizer \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\n\u001b[0;32m    137\u001b[0m seq \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(seq, TokSequence) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[39misinstance\u001b[39;49m(seq_, TokSequence) \u001b[39mfor\u001b[39;49;00m seq_ \u001b[39min\u001b[39;49;00m seq\n\u001b[0;32m    140\u001b[0m ):\n\u001b[0;32m    141\u001b[0m     seq \u001b[39m=\u001b[39m convert_sequence_to_tokseq(tokenizer, seq, complete, decode_bpe)\n\u001b[0;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "tokens_bpe = tokenizer.load_tokens(\n",
    "    \"Maestro_noBPE/MIDI-Unprocessed_XP_20_R2_2004_01_ORIG_MID--AUDIO_20_R1_2004_01_Track01_wav.json\")\n",
    "tokens_bpe_list = list(tokens_bpe.values())\n",
    "\n",
    "tokens = tokenizer.decode_bpe(tokens_bpe_list)\n",
    "midi = tokenizer.tokens_to_midi(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI - Tokens conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "' '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\muze\\REMI.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m midi \u001b[39m=\u001b[39m MidiFile(\u001b[39m\"\u001b[39m\u001b[39mwe_wish_you.mid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer(midi)  \u001b[39m# automatically detects MidiFile, paths\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mlearn_bpe(\u001b[39m10000\u001b[39;49m, [\u001b[39mstr\u001b[39;49m(tokens)])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# MidiTok can handle PyTorch / Tensorflow Tensors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/muze/REMI.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m generated_midi \u001b[39m=\u001b[39m tokenizer(tokens)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\miditok\\midi_tokenizer.py:1634\u001b[0m, in \u001b[0;36mMIDITokenizer.learn_bpe\u001b[1;34m(self, vocab_size, iterator, tokens_paths, start_from_empty_voc, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_to_vocab(\n\u001b[0;32m   1630\u001b[0m                 token, byte_\u001b[39m=\u001b[39mbyte_, add_to_bpe_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m             )  \u001b[39m# adds it to _vocab_base\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[39m# Update __vocab_bpe_bytes_to_tokens for faster decoding\u001b[39;00m\n\u001b[1;32m-> 1634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab_bpe_bytes_to_tokens \u001b[39m=\u001b[39m {\n\u001b[0;32m   1635\u001b[0m     k: [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vocab_base_byte_to_token[b] \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m k]\n\u001b[0;32m   1636\u001b[0m     \u001b[39mfor\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bpe_model\u001b[39m.\u001b[39;49mget_vocab()\n\u001b[0;32m   1637\u001b[0m }\n\u001b[0;32m   1639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_bpe \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\miditok\\midi_tokenizer.py:1635\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_to_vocab(\n\u001b[0;32m   1630\u001b[0m                 token, byte_\u001b[39m=\u001b[39mbyte_, add_to_bpe_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m             )  \u001b[39m# adds it to _vocab_base\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[39m# Update __vocab_bpe_bytes_to_tokens for faster decoding\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab_bpe_bytes_to_tokens \u001b[39m=\u001b[39m {\n\u001b[1;32m-> 1635\u001b[0m     k: [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vocab_base_byte_to_token[b] \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m k]\n\u001b[0;32m   1636\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bpe_model\u001b[39m.\u001b[39mget_vocab()\n\u001b[0;32m   1637\u001b[0m }\n\u001b[0;32m   1639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_bpe \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\miditok\\midi_tokenizer.py:1635\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_to_vocab(\n\u001b[0;32m   1630\u001b[0m                 token, byte_\u001b[39m=\u001b[39mbyte_, add_to_bpe_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m             )  \u001b[39m# adds it to _vocab_base\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[39m# Update __vocab_bpe_bytes_to_tokens for faster decoding\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab_bpe_bytes_to_tokens \u001b[39m=\u001b[39m {\n\u001b[1;32m-> 1635\u001b[0m     k: [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vocab_base_byte_to_token[b] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m k]\n\u001b[0;32m   1636\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bpe_model\u001b[39m.\u001b[39mget_vocab()\n\u001b[0;32m   1637\u001b[0m }\n\u001b[0;32m   1639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_bpe \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "from miditoolkit import MidiFile\n",
    "\n",
    "midi_path = \"we_wish_you.mid\"\n",
    "# Tokenize a MIDI file\n",
    "midi = MidiFile(\"we_wish_you.mid\")\n",
    "tokens = tokenizer(midi)  # automatically detects MidiFile, paths\n",
    "tokenizer.learn_bpe(10000, [str(tokens)])\n",
    "# MidiTok can handle PyTorch / Tensorflow Tensors\n",
    "generated_midi = tokenizer(tokens)\n",
    "# could have been done above by giving the path argument\n",
    "generated_midi.dump('we_wish_you_generated.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert MIDI files to tokens, and load them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\miditok\\midi_tokenizer.py:1800: UserWarning: Tokenizer config file already exists. Overwriting it (Maestro_noBPE\\tokenizer.conf)\n",
      "  warnings.warn(\n",
      "Tokenizing MIDIs (Maestro_noBPE):   0%|          | 0/132 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing MIDIs (Maestro_noBPE): 100%|██████████| 132/132 [01:44<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from miditok import REMI\n",
    "from pathlib import Path\n",
    "\n",
    "midi_paths = list(Path('Maestro').glob('**/*.mid')) + \\\n",
    "    list(Path('Maestro').glob('**/*.midi'))\n",
    "\n",
    "# A validation method to discard MIDIs we do not want\n",
    "# It can also be used for custom pre-processing, for instance if you want to merge\n",
    "# some tracks before tokenizing a MIDI file\n",
    "\n",
    "\n",
    "def midi_valid(midi) -> bool:\n",
    "    if any(ts.numerator != 4 for ts in midi.time_signature_changes):\n",
    "        return False  # time signature different from 4/*, 4 beats per bar\n",
    "    if midi.max_tick < 10 * midi.ticks_per_beat:\n",
    "        return False  # this MIDI is too short\n",
    "    return True\n",
    "\n",
    "\n",
    "tokenizer.tokenize_midi_dataset(midi_paths,\n",
    "                                'Maestro_noBPE', [0, 0.5, 1, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn and apply BPE to data we just tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading token files: 100%|██████████| 132/132 [00:00<00:00, 180.21it/s]\n",
      "Applying BPE to dataset: 100%|██████████| 132/132 [00:21<00:00,  6.24it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_path = Path(\"Maestro_noBPE\")\n",
    "tokens_bpe_path = Path(\"Maestro_BPE\")\n",
    "tokenizer.learn_bpe(\n",
    "    vocab_size=500,\n",
    "    tokens_paths=list(tokens_path.glob(\"**/*.json\")),\n",
    "    start_from_empty_voc=False,\n",
    ")\n",
    "tokenizer.save_params(\"tokenizer_bpe.conf\")\n",
    "tokenizer.apply_bpe_to_dataset(tokens_path, tokens_bpe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding the BPE data to the original midi file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
